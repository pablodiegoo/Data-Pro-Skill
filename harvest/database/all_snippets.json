[
  {
    "id": "weighted_statistics",
    "title": "Weighted Mean and Median",
    "description": "Calculate weighted statistics when observations have different importance or sample sizes",
    "tags": [
      "statistics",
      "weighted",
      "eda",
      "descriptive"
    ],
    "code": "import numpy as np\nimport wquantiles\n\n# Weighted mean\nweighted_mean = np.average(df['value'], weights=df['weight'])\n\n# Weighted median (requires wquantiles package)\nweighted_median = wquantiles.median(df['value'], weights=df['weight'])\n\nprint(f'Weighted Mean: {weighted_mean:.2f}')\nprint(f'Weighted Median: {weighted_median:.2f}')",
    "dependencies": [
      "numpy",
      "wquantiles"
    ]
  },
  {
    "id": "trimmed_mean",
    "title": "Trimmed Mean (Robust Central Tendency)",
    "description": "Calculate mean after removing a percentage of extreme values from both tails",
    "tags": [
      "statistics",
      "robust",
      "outliers",
      "eda"
    ],
    "code": "from scipy.stats import trim_mean\n\n# Remove 10% from each tail (20% total)\ntrimmed_avg = trim_mean(df['value'], proportiontocut=0.1)\n\nprint(f'Trimmed Mean (10% each tail): {trimmed_avg:.2f}')\nprint(f'Regular Mean: {df[\"value\"].mean():.2f}')\nprint(f'Difference: {abs(trimmed_avg - df[\"value\"].mean()):.2f}')",
    "dependencies": [
      "scipy"
    ]
  },
  {
    "id": "mad_statistic",
    "title": "Median Absolute Deviation (MAD)",
    "description": "Robust measure of variability that is resistant to outliers",
    "tags": [
      "statistics",
      "robust",
      "variability",
      "outliers"
    ],
    "code": "from statsmodels import robust\n\n# Using statsmodels\nmad_value = robust.scale.mad(df['value'])\n\n# Manual calculation\nmedian_val = df['value'].median()\nmad_manual = abs(df['value'] - median_val).median() / 0.6744897501960817\n\nprint(f'MAD: {mad_value:.2f}')\nprint(f'Standard Deviation: {df[\"value\"].std():.2f}')\nprint(f'MAD is more robust to outliers')",
    "dependencies": [
      "statsmodels"
    ]
  },
  {
    "id": "permutation_test",
    "title": "Permutation Test (Two Groups)",
    "description": "Non-parametric hypothesis test comparing two groups without assuming normality",
    "tags": [
      "statistics",
      "hypothesis-testing",
      "non-parametric",
      "permutation"
    ],
    "code": "import numpy as np\nimport random\n\ndef permutation_test(group_a, group_b, n_permutations=10000):\n    # Observed difference\n    observed_diff = np.mean(group_b) - np.mean(group_a)\n    \n    # Combine groups\n    combined = np.concatenate([group_a, group_b])\n    n_a, n_b = len(group_a), len(group_b)\n    \n    # Permutation distribution\n    perm_diffs = []\n    for _ in range(n_permutations):\n        shuffled = np.random.permutation(combined)\n        perm_a = shuffled[:n_a]\n        perm_b = shuffled[n_a:]\n        perm_diffs.append(np.mean(perm_b) - np.mean(perm_a))\n    \n    # P-value\n    p_value = np.mean(np.abs(perm_diffs) >= np.abs(observed_diff))\n    \n    return {'observed_diff': observed_diff, 'p_value': p_value}\n\nresult = permutation_test(df[df['group']=='A']['value'].values, \n                          df[df['group']=='B']['value'].values)\nprint(f\"Observed difference: {result['observed_diff']:.2f}\")\nprint(f\"P-value: {result['p_value']:.4f}\")",
    "dependencies": [
      "numpy"
    ]
  },
  {
    "id": "correlation_ellipse",
    "title": "Correlation Matrix with Ellipses",
    "description": "Visualize correlation matrix using ellipses (grayscale-friendly)",
    "tags": [
      "visualization",
      "correlation",
      "grayscale",
      "publication"
    ],
    "code": "from matplotlib.collections import EllipseCollection\nfrom matplotlib.colors import Normalize\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef plot_corr_ellipses(data, figsize=None, **kwargs):\n    M = np.array(data)\n    fig, ax = plt.subplots(1, 1, figsize=figsize, subplot_kw={'aspect': 'equal'})\n    ax.set_xlim(-0.5, M.shape[1] - 0.5)\n    ax.set_ylim(-0.5, M.shape[0] - 0.5)\n    ax.invert_yaxis()\n    xy = np.indices(M.shape)[::-1].reshape(2, -1).T\n    w = np.ones_like(M).ravel() + 0.01\n    h = 1 - np.abs(M).ravel() - 0.01\n    a = 45 * np.sign(M).ravel()\n    ec = EllipseCollection(widths=w, heights=h, angles=a, units='x', offsets=xy,\n                          norm=Normalize(vmin=-1, vmax=1),\n                          transOffset=ax.transData, array=M.ravel(), **kwargs)\n    ax.add_collection(ec)\n    if isinstance(data, pd.DataFrame):\n        ax.set_xticks(np.arange(M.shape[1]))\n        ax.set_xticklabels(data.columns, rotation=90)\n        ax.set_yticks(np.arange(M.shape[0]))\n        ax.set_yticklabels(data.index)\n    return ec, ax\n\ncorr_matrix = df.corr()\nm, ax = plot_corr_ellipses(corr_matrix, figsize=(8, 8), cmap='bwr_r')\nplt.colorbar(m, ax=ax, label='Correlation coefficient')\nplt.show()",
    "dependencies": [
      "matplotlib",
      "numpy",
      "pandas"
    ]
  },
  {
    "id": "partial_residual_plot",
    "title": "Partial Residual Plot",
    "description": "Diagnostic plot for detecting nonlinearity in individual predictors",
    "tags": [
      "regression",
      "diagnostics",
      "nonlinearity",
      "visualization"
    ],
    "code": "import statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# For linear terms, use statsmodels built-in\nfig, ax = plt.subplots(figsize=(8, 6))\nsm.graphics.plot_ccpr(model_result, 'predictor_name', ax=ax)\nplt.title('Partial Residual Plot')\nplt.show()\n\n# For polynomial terms, use custom implementation\n# See assets/harvest/scripts/partial_residual_plot.py",
    "dependencies": [
      "statsmodels",
      "matplotlib"
    ]
  },
  {
    "id": "pca_scree_plot",
    "title": "PCA Scree Plot",
    "description": "Visualize explained variance ratio for Principal Component Analysis",
    "tags": [
      "pca",
      "dimensionality-reduction",
      "visualization",
      "variance"
    ],
    "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\nvar_exp = pca.explained_variance_ratio_\ncum_var_exp = np.cumsum(var_exp)\n\nplt.bar(range(1, len(var_exp) + 1), var_exp, alpha=0.5, align='center', label='Individual')\nplt.step(range(1, len(var_exp) + 1), cum_var_exp, where='mid', label='Cumulative')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()",
    "dependencies": [
      "matplotlib",
      "numpy"
    ]
  },
  {
    "id": "pca_loadings_chart",
    "title": "PCA Component Loadings",
    "description": "Visualize feature contributions (loadings) to principal components",
    "tags": [
      "pca",
      "interpretation",
      "visualization",
      "loadings"
    ],
    "code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Get loadings for first 2 components\nloadings = pd.DataFrame(pca.components_[0:2].T, \n                        index=df.columns, \n                        columns=['PC1', 'PC2'])\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\nloadings['PC1'].plot.bar(ax=ax1, color='blue')\nloadings['PC2'].plot.bar(ax=ax2, color='red')\nax1.set_ylabel('PC1 Loading')\nax2.set_ylabel('PC2 Loading')\nplt.tight_layout()\nplt.show()",
    "dependencies": [
      "pandas",
      "matplotlib"
    ]
  },
  {
    "id": "gower_distance_mixed",
    "title": "Clustering Mixed Data (Gower)",
    "description": "Use Gower distance for clustering datasets with mixed categorical and numeric types",
    "tags": [
      "clustering",
      "mixed-data",
      "gower",
      "unsupervised"
    ],
    "code": "import gower\nfrom scipy.cluster.hierarchy import linkage, fcluster\nfrom scipy.spatial.distance import squareform\n\n# Calculate Gower distance matrix\ndist_matrix = gower.gower_matrix(df)\n\n# Hierarchical clustering\nZ = linkage(squareform(dist_matrix), method='complete')\nclusters = fcluster(Z, 4, criterion='maxclust')\n\ndf['cluster'] = clusters\nprint(df.groupby('cluster').mean())",
    "dependencies": [
      "gower",
      "scipy",
      "pandas"
    ]
  },
  {
    "id": "quantile_residual_grouping",
    "title": "High-Cardinality Grouping",
    "description": "Group high-cardinality categorical levels (like zip codes) into quantiles based on model residuals",
    "tags": [
      "feature-engineering",
      "categorical",
      "regression",
      "grouping"
    ],
    "code": "import pandas as pd\nimport numpy as np\n\n# 1. Calculate residuals for initial small model\nresiduals = y - model.predict(X)\n\n# 2. Group by categorical level and get median residual\nresid_df = pd.DataFrame({'category': df['zip_code'], 'resid': residuals})\nmedian_resids = resid_df.groupby('category')['resid'].median().sort_values()\n\n# 3. Create quantiles (e.g., quintiles)\nmedian_resids_df = median_resids.to_frame()\nmedian_resids_df['group'] = pd.qcut(median_resids_df['resid'], 5, labels=False)\n\n# 4. Map back to original dataframe\ndf['zip_group'] = df['zip_code'].map(median_resids_df['group'])",
    "dependencies": [
      "pandas",
      "numpy"
    ]
  }
]