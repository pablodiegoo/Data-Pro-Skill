#!/usr/bin/env python3
"""
Data Pro Max - Reasoning Engine
Analyzes data profiles and generates intelligent analysis plans.

Usage:
    python3 reasoning_engine.py data.csv
    python3 reasoning_engine.py data.csv --goal "customer segmentation"
    python3 reasoning_engine.py data.csv --domain survey --output plan.md
"""

import argparse
import csv
import json
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional

import pandas as pd
import numpy as np

# Paths
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR.parent / "data"

# Knowledge base files
ANALYSIS_TYPES = DATA_DIR / "analysis_types.csv"
VISUALIZATION_RULES = DATA_DIR / "visualization_rules.csv"
PALETTES = DATA_DIR / "palettes.csv"
REASONING_RULES = DATA_DIR / "reasoning_rules.csv"


@dataclass
class DataProfile:
    """Profile of the dataset characteristics."""
    n_rows: int = 0
    n_cols: int = 0
    numeric_cols: list = field(default_factory=list)
    categorical_cols: list = field(default_factory=list)
    datetime_cols: list = field(default_factory=list)
    text_cols: list = field(default_factory=list)
    missing_pct: dict = field(default_factory=dict)
    skewness: dict = field(default_factory=dict)
    has_likert: bool = False
    has_ordinal: bool = False
    n_groups: int = 0
    is_timeseries: bool = False
    has_weight_col: bool = False
    domain: str = "general"
    goal: str = ""
    

@dataclass
class Recommendation:
    """A single recommendation from the reasoning engine."""
    priority: str
    category: str
    title: str
    description: str
    action: str
    rationale: str
    

@dataclass  
class AnalysisPlan:
    """Complete analysis plan generated by the engine."""
    data_profile: DataProfile
    recommended_analyses: list = field(default_factory=list)
    recommended_visualizations: list = field(default_factory=list)
    recommended_palette: dict = field(default_factory=dict)
    warnings: list = field(default_factory=list)
    checklist: list = field(default_factory=list)


def load_csv(filepath: Path) -> list[dict]:
    """Load CSV file into list of dicts."""
    if not filepath.exists():
        return []
    with open(filepath, newline="", encoding="utf-8") as f:
        return list(csv.DictReader(f))


def profile_data(df: pd.DataFrame, domain: str = "general", goal: str = "") -> DataProfile:
    """
    Analyze DataFrame and create a profile of its characteristics.
    """
    profile = DataProfile(
        n_rows=len(df),
        n_cols=len(df.columns),
        domain=domain,
        goal=goal,
    )
    
    for col in df.columns:
        # Check for missing data
        missing = df[col].isna().sum() / len(df) * 100
        if missing > 0:
            profile.missing_pct[col] = round(missing, 1)
        
        # Classify column type
        dtype = df[col].dtype
        
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            profile.datetime_cols.append(col)
            profile.is_timeseries = True
        elif pd.api.types.is_numeric_dtype(df[col]):
            profile.numeric_cols.append(col)
            # Check skewness
            try:
                skew = df[col].skew()
                if abs(skew) > 0.5:
                    profile.skewness[col] = round(skew, 2)
            except:
                pass
            # Check for Likert-like (1-5, 1-7, 1-10 scales)
            unique = df[col].dropna().unique()
            if len(unique) <= 10 and df[col].min() >= 0 and df[col].max() <= 10:
                profile.has_likert = True
                profile.has_ordinal = True
        elif pd.api.types.is_object_dtype(df[col]):
            unique = df[col].nunique()
            if unique <= 20:
                profile.categorical_cols.append(col)
                profile.n_groups = max(profile.n_groups, unique)
            else:
                # Likely text
                avg_len = df[col].str.len().mean() if df[col].str.len().mean() else 0
                if avg_len > 50:
                    profile.text_cols.append(col)
                else:
                    profile.categorical_cols.append(col)
    
    # Check for weight column
    weight_keywords = ["weight", "peso", "ponderacao", "wgt", "factor"]
    for col in df.columns:
        if any(kw in col.lower() for kw in weight_keywords):
            profile.has_weight_col = True
            break
    
    return profile


def apply_reasoning_rules(profile: DataProfile, rules: list[dict]) -> list[Recommendation]:
    """
    Apply reasoning rules to the data profile and generate recommendations.
    """
    recommendations = []
    
    for rule in rules:
        triggered = False
        trigger = rule.get("trigger", "")
        condition = rule.get("condition", "")
        
        # Check trigger conditions
        if trigger == "data_type":
            if "has_likert" in condition and profile.has_likert:
                triggered = True
        
        elif trigger == "distribution":
            if "skewness" in condition:
                for col, skew in profile.skewness.items():
                    if abs(skew) > 2:
                        triggered = True
                        break
        
        elif trigger == "comparison":
            if "n_groups >= 3" in condition and profile.n_groups >= 3:
                triggered = True
            elif "n_groups == 2" in condition and profile.n_groups == 2:
                triggered = True
        
        elif trigger == "missing_data":
            if "missing_pct > 5" in condition:
                for col, pct in profile.missing_pct.items():
                    if pct > 5:
                        triggered = True
                        break
            elif "missing_pct > 30" in condition:
                for col, pct in profile.missing_pct.items():
                    if pct > 30:
                        triggered = True
                        break
        
        elif trigger == "sample_size":
            if "n < 30" in condition and profile.n_rows < 30:
                triggered = True
            elif "n < 100" in condition and profile.n_rows < 100 and profile.has_likert:
                triggered = True
        
        elif trigger == "correlation":
            if "has_ordinal" in condition and profile.has_ordinal:
                triggered = True
        
        elif trigger == "time_series":
            if "has_datetime" in condition and profile.is_timeseries:
                triggered = True
        
        elif trigger == "weighting":
            if "is_survey" in condition and profile.domain == "survey":
                triggered = True
            elif "has_weight" in condition and profile.has_weight_col:
                triggered = True
        
        elif trigger == "text_analysis":
            if "has_open_ended" in condition and len(profile.text_cols) > 0:
                triggered = True
        
        elif trigger == "survey_type":
            if profile.domain == "survey":
                triggered = True
        
        if triggered:
            recommendations.append(Recommendation(
                priority=rule.get("priority", "medium"),
                category=trigger,
                title=rule.get("recommendation", ""),
                description=rule.get("recommendation", ""),
                action=rule.get("action", ""),
                rationale=rule.get("rationale", ""),
            ))
    
    # Sort by priority
    priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
    recommendations.sort(key=lambda x: priority_order.get(x.priority, 99))
    
    return recommendations


def recommend_analyses(profile: DataProfile, analyses: list[dict]) -> list[dict]:
    """
    Recommend analysis types based on data profile.
    """
    recommended = []
    
    for analysis in analyses:
        score = 0
        
        # Match domain
        if analysis.get("domain") == profile.domain:
            score += 2
        elif analysis.get("domain") == "general":
            score += 1
        
        # Match data characteristics
        category = analysis.get("category", "")
        
        if category == "descriptive":
            score += 1  # Always start with descriptive
        
        if category == "inferential" and profile.n_groups >= 2:
            score += 2
        
        if category == "dimensionality" and profile.has_likert and len(profile.numeric_cols) >= 5:
            score += 3
        
        if category == "segmentation" and "segment" in profile.goal.lower():
            score += 4
        
        if category == "nlp" and len(profile.text_cols) > 0:
            score += 3
        
        if category == "timeseries" and profile.is_timeseries:
            score += 3
        
        if score >= 2:
            recommended.append({
                **analysis,
                "_score": score
            })
    
    # Sort by score
    recommended.sort(key=lambda x: x["_score"], reverse=True)
    return recommended[:10]


def recommend_visualizations(profile: DataProfile, viz_rules: list[dict]) -> list[dict]:
    """
    Recommend visualizations based on data profile.
    """
    recommended = []
    
    for viz in viz_rules:
        score = 0
        data_type = viz.get("data_type", "")
        
        if data_type == "categorical" and len(profile.categorical_cols) > 0:
            score += 2
        
        if data_type == "numeric" and len(profile.numeric_cols) > 0:
            score += 2
        
        if data_type == "timeseries" and profile.is_timeseries:
            score += 3
        
        if data_type == "text" and len(profile.text_cols) > 0:
            score += 2
        
        if score > 0:
            recommended.append({
                **viz,
                "_score": score
            })
    
    recommended.sort(key=lambda x: x["_score"], reverse=True)
    return recommended[:8]


def recommend_palette(profile: DataProfile, palettes: list[dict]) -> dict:
    """
    Recommend a color palette based on domain.
    """
    for palette in palettes:
        if palette.get("domain") == profile.domain:
            return palette
    
    # Default to neutral
    for palette in palettes:
        if palette.get("id") == "neutral_gray":
            return palette
    
    return palettes[0] if palettes else {}


def generate_analysis_plan(
    df: pd.DataFrame,
    domain: str = "general",
    goal: str = "",
) -> AnalysisPlan:
    """
    Main function: Generate a complete analysis plan.
    """
    # Load knowledge base
    analyses = load_csv(ANALYSIS_TYPES)
    viz_rules = load_csv(VISUALIZATION_RULES)
    palettes = load_csv(PALETTES)
    rules = load_csv(REASONING_RULES)
    
    # Profile the data
    profile = profile_data(df, domain, goal)
    
    # Generate recommendations
    reasoning_recs = apply_reasoning_rules(profile, rules)
    analysis_recs = recommend_analyses(profile, analyses)
    viz_recs = recommend_visualizations(profile, viz_rules)
    palette = recommend_palette(profile, palettes)
    
    # Build warnings
    warnings = []
    for col, pct in profile.missing_pct.items():
        if pct > 5:
            warnings.append(f"⚠️ Column '{col}' has {pct}% missing values")
    
    for col, skew in profile.skewness.items():
        if abs(skew) > 2:
            warnings.append(f"⚠️ Column '{col}' is highly skewed (skew={skew})")
    
    if profile.n_rows < 30:
        warnings.append("⚠️ Small sample size (<30) - interpret results with caution")
    
    # Build checklist
    checklist = [
        "[ ] Verify data quality and completeness",
        "[ ] Document missing data handling strategy",
        "[ ] Check statistical assumptions",
        "[ ] Report effect sizes alongside p-values",
        "[ ] Use appropriate visualization types",
        "[ ] Apply domain-appropriate color palette",
        "[ ] Review results for actionable insights",
    ]
    
    return AnalysisPlan(
        data_profile=profile,
        recommended_analyses=analysis_recs,
        recommended_visualizations=viz_recs,
        recommended_palette=palette,
        warnings=warnings,
        checklist=checklist,
    )


def format_plan_markdown(plan: AnalysisPlan) -> str:
    """
    Format analysis plan as Markdown.
    """
    p = plan.data_profile
    
    md = f"""# Data Analysis Plan

## Data Profile

| Metric | Value |
|--------|-------|
| Rows | {p.n_rows:,} |
| Columns | {p.n_cols} |
| Numeric | {len(p.numeric_cols)} |
| Categorical | {len(p.categorical_cols)} |
| Text | {len(p.text_cols)} |
| DateTime | {len(p.datetime_cols)} |
| Domain | {p.domain} |
| Goal | {p.goal or 'Not specified'} |

"""

    if plan.warnings:
        md += "## ⚠️ Warnings\n\n"
        for w in plan.warnings:
            md += f"- {w}\n"
        md += "\n"
    
    md += "## Recommended Analyses\n\n"
    for i, a in enumerate(plan.recommended_analyses[:5], 1):
        md += f"""### {i}. {a.get('name', '')}
- **Category**: {a.get('category', '')}
- **Use Case**: {a.get('use_case', '')}
- **When to Use**: {a.get('when_to_use', '')}
- **Python**: `{a.get('python_function', '')}`

"""
    
    md += "## Recommended Visualizations\n\n"
    for v in plan.recommended_visualizations[:5]:
        md += f"- **{v.get('chart_type', '')}**: {v.get('best_for', '')} (`{v.get('plotly_type', '')}`)\n"
    md += "\n"
    
    pal = plan.recommended_palette
    if pal:
        md += f"""## Recommended Palette: {pal.get('name', '')}

- **Domain**: {pal.get('domain', '')}
- **Mood**: {pal.get('mood', '')}
- **Colors**: {pal.get('color_primary', '')} | {pal.get('color_secondary', '')} | {pal.get('color_accent', '')}
- **Seaborn**: `{pal.get('seaborn_palette', '')}`

"""

    md += "## Pre-Analysis Checklist\n\n"
    for item in plan.checklist:
        md += f"- {item}\n"
    
    return md


def main():
    parser = argparse.ArgumentParser(
        description="Generate data analysis plan from dataset",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument("datafile", help="CSV file to analyze")
    parser.add_argument("--domain", "-d", default="general", 
                        choices=["general", "survey", "research", "marketing", "healthcare", "financial"],
                        help="Domain context")
    parser.add_argument("--goal", "-g", default="", help="Analysis goal description")
    parser.add_argument("--output", "-o", help="Output file (default: stdout)")
    parser.add_argument("--json", "-j", action="store_true", help="Output as JSON")
    parser.add_argument("--rows", "-n", type=int, default=1000, help="Max rows to read for profiling")

    args = parser.parse_args()

    # Load data
    try:
        df = pd.read_csv(args.datafile, nrows=args.rows)
    except Exception as e:
        print(f"Error loading {args.datafile}: {e}", file=sys.stderr)
        sys.exit(1)

    # Generate plan
    plan = generate_analysis_plan(df, args.domain, args.goal)

    # Format output
    if args.json:
        output = json.dumps({
            "profile": {
                "n_rows": plan.data_profile.n_rows,
                "n_cols": plan.data_profile.n_cols,
                "numeric_cols": plan.data_profile.numeric_cols,
                "categorical_cols": plan.data_profile.categorical_cols,
                "text_cols": plan.data_profile.text_cols,
                "missing_pct": plan.data_profile.missing_pct,
                "domain": plan.data_profile.domain,
            },
            "recommended_analyses": [a["name"] for a in plan.recommended_analyses],
            "recommended_visualizations": [v["chart_type"] for v in plan.recommended_visualizations],
            "palette": plan.recommended_palette.get("name", ""),
            "warnings": plan.warnings,
        }, indent=2)
    else:
        output = format_plan_markdown(plan)

    # Write output
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            f.write(output)
        print(f"Plan saved to: {args.output}")
    else:
        print(output)


if __name__ == "__main__":
    main()
