{
  "desc_univar": {
    "id": "desc_univar",
    "name": "Univariate Descriptive",
    "category": "descriptive",
    "description": "Summarize single variable distribution",
    "imports": "import pandas as pd\nimport numpy as np",
    "code": "def univariate_summary(df, col):\n    \"\"\"Generate univariate statistics for a column.\"\"\"\n    stats = {\n        'count': df[col].count(),\n        'missing': df[col].isna().sum(),\n        'missing_pct': round(df[col].isna().mean() * 100, 1),\n        'unique': df[col].nunique()\n    }\n    if pd.api.types.is_numeric_dtype(df[col]):\n        stats.update({\n            'mean': round(df[col].mean(), 2),\n            'std': round(df[col].std(), 2),\n            'min': df[col].min(),\n            'median': df[col].median(),\n            'max': df[col].max(),\n            'skew': round(df[col].skew(), 2)\n        })\n    else:\n        stats['top_values'] = df[col].value_counts().head(5).to_dict()\n    return stats",
    "output_type": "dict",
    "example": "result = univariate_summary(df, 'satisfaction')"
  },
  "cross_tab": {
    "id": "cross_tab",
    "name": "Cross Tabulation",
    "category": "descriptive",
    "description": "Calculate frequency tables between two categorical variables",
    "imports": "import pandas as pd",
    "code": "def cross_tabulation(df, row_var, col_var, normalize=None):\n    \"\"\"Create cross-tabulation with optional normalization.\n    normalize: None, 'index', 'columns', 'all'\n    \"\"\"\n    ct = pd.crosstab(df[row_var], df[col_var], margins=True, normalize=normalize)\n    return ct",
    "output_type": "DataFrame",
    "example": "ct = cross_tabulation(df, 'gender', 'region', normalize='index')"
  },
  "chi_square": {
    "id": "chi_square",
    "name": "Chi-Square Test",
    "category": "inferential",
    "description": "Test independence between categorical variables",
    "imports": "from scipy import stats\nimport pandas as pd\nimport numpy as np",
    "code": "def chi_square_test(df, var1, var2):\n    \"\"\"Perform chi-square test of independence.\"\"\"\n    contingency = pd.crosstab(df[var1], df[var2])\n    chi2, p, dof, expected = stats.chi2_contingency(contingency)\n    n = len(df)\n    cramers_v = np.sqrt(chi2 / (n * (min(contingency.shape) - 1)))\n    return {\n        'chi2': round(chi2, 4),\n        'p_value': round(p, 4),\n        'dof': dof,\n        'cramers_v': round(cramers_v, 4),\n        'significant': p < 0.05\n    }",
    "output_type": "dict",
    "example": "result = chi_square_test(df, 'gender', 'preference')"
  },
  "ttest_ind": {
    "id": "ttest_ind",
    "name": "Independent T-Test",
    "category": "inferential",
    "description": "Compare means between two groups",
    "imports": "from scipy import stats\nimport numpy as np",
    "code": "def independent_ttest(group1, group2, alpha=0.05):\n    \"\"\"Perform independent samples t-test with effect size.\"\"\"\n    t_stat, p_value = stats.ttest_ind(group1, group2)\n    pooled_std = np.sqrt(((len(group1)-1)*np.var(group1) + (len(group2)-1)*np.var(group2)) / (len(group1)+len(group2)-2))\n    cohens_d = (np.mean(group1) - np.mean(group2)) / pooled_std\n    return {\n        't_statistic': round(t_stat, 4),\n        'p_value': round(p_value, 4),\n        'cohens_d': round(cohens_d, 4),\n        'significant': p_value < alpha,\n        'effect': 'small' if abs(cohens_d) < 0.5 else 'medium' if abs(cohens_d) < 0.8 else 'large'\n    }",
    "output_type": "dict",
    "example": "result = independent_ttest(df[df['group']=='A']['score'], df[df['group']=='B']['score'])"
  },
  "mann_whitney": {
    "id": "mann_whitney",
    "name": "Mann-Whitney U Test",
    "category": "inferential",
    "description": "Non-parametric alternative to t-test",
    "imports": "from scipy import stats",
    "code": "def mann_whitney_test(group1, group2, alpha=0.05):\n    \"\"\"Perform Mann-Whitney U test (non-parametric).\"\"\"\n    stat, p_value = stats.mannwhitneyu(group1, group2, alternative='two-sided')\n    n1, n2 = len(group1), len(group2)\n    effect_size = 1 - (2*stat) / (n1 * n2)\n    return {\n        'u_statistic': round(stat, 4),\n        'p_value': round(p_value, 4),\n        'effect_size': round(abs(effect_size), 4),\n        'significant': p_value < alpha\n    }",
    "output_type": "dict",
    "example": "result = mann_whitney_test(group_a, group_b)"
  },
  "anova_oneway": {
    "id": "anova_oneway",
    "name": "One-Way ANOVA",
    "category": "inferential",
    "description": "Compare means across 3+ groups",
    "imports": "from scipy import stats\nimport numpy as np",
    "code": "def oneway_anova(*groups):\n    \"\"\"Perform one-way ANOVA with effect size.\"\"\"\n    f_stat, p_value = stats.f_oneway(*groups)\n    all_data = np.concatenate(groups)\n    grand_mean = np.mean(all_data)\n    ss_between = sum(len(g) * (np.mean(g) - grand_mean)**2 for g in groups)\n    ss_total = sum((x - grand_mean)**2 for x in all_data)\n    eta_squared = ss_between / ss_total\n    return {\n        'f_statistic': round(f_stat, 4),\n        'p_value': round(p_value, 4),\n        'eta_squared': round(eta_squared, 4),\n        'significant': p_value < 0.05\n    }",
    "output_type": "dict",
    "example": "result = oneway_anova(group_a, group_b, group_c)"
  },
  "correlation_pearson": {
    "id": "correlation_pearson",
    "name": "Pearson Correlation",
    "category": "correlation",
    "description": "Measure linear relationship between numeric variables",
    "imports": "from scipy import stats",
    "code": "def pearson_correlation(df, var1, var2):\n    \"\"\"Calculate Pearson correlation with significance.\"\"\"\n    data = df[[var1, var2]].dropna()\n    r, p = stats.pearsonr(data[var1], data[var2])\n    return {\n        'correlation': round(r, 4),\n        'p_value': round(p, 4),\n        'strength': 'weak' if abs(r) < 0.3 else 'moderate' if abs(r) < 0.7 else 'strong',\n        'significant': p < 0.05\n    }",
    "output_type": "dict",
    "example": "result = pearson_correlation(df, 'age', 'income')"
  },
  "correlation_spearman": {
    "id": "correlation_spearman",
    "name": "Spearman Correlation",
    "category": "correlation",
    "description": "Measure monotonic relationship for ordinal data",
    "imports": "from scipy import stats",
    "code": "def spearman_correlation(df, var1, var2):\n    \"\"\"Calculate Spearman correlation for ordinal data.\"\"\"\n    data = df[[var1, var2]].dropna()\n    rho, p = stats.spearmanr(data[var1], data[var2])\n    return {\n        'correlation': round(rho, 4),\n        'p_value': round(p, 4),\n        'significant': p < 0.05\n    }",
    "output_type": "dict",
    "example": "result = spearman_correlation(df, 'satisfaction', 'loyalty')"
  },
  "nps_calc": {
    "id": "nps_calc",
    "name": "NPS Calculation",
    "category": "survey",
    "description": "Calculate Net Promoter Score",
    "imports": "import pandas as pd",
    "code": "def calculate_nps(df, nps_column):\n    \"\"\"Calculate Net Promoter Score from 0-10 scale.\"\"\"\n    promoters = (df[nps_column] >= 9).sum()\n    detractors = (df[nps_column] <= 6).sum()\n    total = df[nps_column].notna().sum()\n    nps = ((promoters - detractors) / total) * 100\n    return {\n        'nps': round(nps, 1),\n        'promoters_pct': round(promoters/total*100, 1),\n        'passives_pct': round((total-promoters-detractors)/total*100, 1),\n        'detractors_pct': round(detractors/total*100, 1),\n        'n': total\n    }",
    "output_type": "dict",
    "example": "result = calculate_nps(df, 'recommend_score')"
  },
  "top_box": {
    "id": "top_box",
    "name": "Top-Box Score",
    "category": "survey",
    "description": "Calculate top-box percentage for satisfaction scales",
    "imports": "import pandas as pd",
    "code": "def top_box_score(df, column, top_values=[4, 5], scale_max=5):\n    \"\"\"Calculate top-box score (% selecting top values).\"\"\"\n    valid = df[column].dropna()\n    top_box = valid.isin(top_values).sum()\n    return {\n        'top_box_pct': round(top_box / len(valid) * 100, 1),\n        'top_2_box': round((valid >= scale_max - 1).sum() / len(valid) * 100, 1),\n        'mean': round(valid.mean(), 2),\n        'n': len(valid)\n    }",
    "output_type": "dict",
    "example": "result = top_box_score(df, 'satisfaction')"
  },
  "weighted_freq": {
    "id": "weighted_freq",
    "name": "Weighted Frequency",
    "category": "survey",
    "description": "Calculate weighted frequencies for survey data",
    "imports": "import pandas as pd",
    "code": "def weighted_frequency(df, column, weight_column):\n    \"\"\"Calculate weighted frequencies.\"\"\"\n    result = df.groupby(column).apply(\n        lambda x: x[weight_column].sum()\n    ).reset_index(name='weighted_count')\n    result['weighted_pct'] = result['weighted_count'] / result['weighted_count'].sum() * 100\n    return result.round(2)",
    "output_type": "DataFrame",
    "example": "result = weighted_frequency(df, 'satisfaction', 'weight')"
  },
  "factor_analysis": {
    "id": "factor_analysis",
    "name": "Factor Analysis",
    "category": "dimensionality",
    "description": "Identify latent constructs from Likert scales",
    "imports": "from factor_analyzer import FactorAnalyzer\nimport pandas as pd",
    "code": "def run_factor_analysis(df, columns, n_factors=3, rotation='varimax'):\n    \"\"\"Perform factor analysis on survey items.\"\"\"\n    data = df[columns].dropna()\n    fa = FactorAnalyzer(n_factors=n_factors, rotation=rotation)\n    fa.fit(data)\n    loadings = pd.DataFrame(\n        fa.loadings_,\n        index=columns,\n        columns=[f'Factor_{i+1}' for i in range(n_factors)]\n    )\n    variance = fa.get_factor_variance()\n    return {\n        'loadings': loadings.round(3),\n        'variance_explained': variance[1].round(3).tolist(),\n        'total_variance': round(sum(variance[1]) * 100, 1)\n    }",
    "output_type": "dict",
    "example": "result = run_factor_analysis(df, ['q1', 'q2', 'q3', 'q4', 'q5'])"
  },
  "kmeans_segment": {
    "id": "kmeans_segment",
    "name": "K-Means Segmentation",
    "category": "segmentation",
    "description": "Cluster respondents into segments",
    "imports": "from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd",
    "code": "def kmeans_segmentation(df, columns, n_clusters=4):\n    \"\"\"Perform K-Means clustering for segmentation.\"\"\"\n    data = df[columns].dropna()\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(data)\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    clusters = kmeans.fit_predict(scaled)\n    df_result = data.copy()\n    df_result['segment'] = clusters\n    profiles = df_result.groupby('segment')[columns].mean().round(2)\n    return {\n        'labels': clusters,\n        'profiles': profiles,\n        'inertia': round(kmeans.inertia_, 2)\n    }",
    "output_type": "dict",
    "example": "result = kmeans_segmentation(df, ['factor1', 'factor2', 'factor3'])"
  },
  "normality_test": {
    "id": "normality_test",
    "name": "Normality Test",
    "category": "assumption",
    "description": "Test if data is normally distributed",
    "imports": "from scipy import stats",
    "code": "def test_normality(data, alpha=0.05):\n    \"\"\"Test normality using Shapiro-Wilk test.\"\"\"\n    sample = data.dropna()\n    if len(sample) > 5000:\n        sample = sample.sample(5000, random_state=42)\n    stat, p = stats.shapiro(sample)\n    return {\n        'statistic': round(stat, 4),\n        'p_value': round(p, 4),\n        'normal': p >= alpha,\n        'recommendation': 'Use parametric tests' if p >= alpha else 'Use non-parametric tests'\n    }",
    "output_type": "dict",
    "example": "result = test_normality(df['score'])"
  },
  "cohens_d": {
    "id": "cohens_d",
    "name": "Cohen's d Effect Size",
    "category": "effect_size",
    "description": "Calculate standardized effect size",
    "imports": "import numpy as np",
    "code": "def cohens_d(group1, group2):\n    \"\"\"Calculate Cohen's d effect size.\"\"\"\n    n1, n2 = len(group1), len(group2)\n    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n    d = (np.mean(group1) - np.mean(group2)) / pooled_std\n    return {\n        'cohens_d': round(d, 4),\n        'interpretation': 'small' if abs(d) < 0.5 else 'medium' if abs(d) < 0.8 else 'large'\n    }",
    "output_type": "dict",
    "example": "result = cohens_d(treatment_scores, control_scores)"
  },
  "melt_data": {
    "id": "melt_data",
    "name": "Wide to Long (Melt)",
    "category": "transformation",
    "description": "Convert wide table to long format (Tidy Data) for visualization",
    "imports": "import pandas as pd",
    "code": "def tidy_melt(df, id_vars, value_vars=None, var_name='question', value_name='score'):\n    \"\"\"Convert wide dataframe to long format.\n    id_vars: columns to keep fixed (e.g. ['id', 'gender'])\n    value_vars: columns to melt (default: all others)\n    \"\"\"\n    return pd.melt(\n        df, \n        id_vars=id_vars, \n        value_vars=value_vars, \n        var_name=var_name, \n        value_name=value_name\n    )",
    "output_type": "DataFrame",
    "example": "long_df = tidy_melt(df, id_vars=['respondent_id'], value_vars=['q1', 'q2', 'q3'])"
  },
  "missing_report": {
    "id": "missing_report",
    "name": "Missing Data Report",
    "category": "data_quality",
    "description": "Generate a summary of missing values with decision thresholds (<5% drop rows, 5-30% impute, >30% consider dropping variable)",
    "imports": "import pandas as pd",
    "code": "def missing_data_report(df):\n    \"\"\"Generate a missing data report with action recommendations.\"\"\"\n    missing = df.isna().sum().sort_values(ascending=False)\n    pct = (missing / len(df) * 100).round(1)\n    report = pd.DataFrame({'missing_count': missing, 'missing_pct': pct})\n    report['action'] = report['missing_pct'].apply(\n        lambda x: 'OK - listwise delete' if x < 5 else 'Consider imputation' if x < 30 else 'Consider dropping'\n    )\n    return report[report['missing_count'] > 0]",
    "output_type": "DataFrame",
    "example": "report = missing_data_report(df)"
  },
  "weighted_statistics": {
    "id": "weighted_statistics",
    "name": "Weighted Mean and Median",
    "category": "statistics",
    "description": "Calculate weighted statistics when observations have different importance or sample sizes",
    "imports": "",
    "code": "import numpy as np\nimport wquantiles\n\n# Weighted mean\nweighted_mean = np.average(df['value'], weights=df['weight'])\n\n# Weighted median (requires wquantiles package)\nweighted_median = wquantiles.median(df['value'], weights=df['weight'])\n\nprint(f'Weighted Mean: {weighted_mean:.2f}')\nprint(f'Weighted Median: {weighted_median:.2f}')",
    "output_type": "None",
    "example": "# See snippet weighted_statistics"
  },
  "trimmed_mean": {
    "id": "trimmed_mean",
    "name": "Trimmed Mean (Robust Central Tendency)",
    "category": "statistics",
    "description": "Calculate mean after removing a percentage of extreme values from both tails",
    "imports": "",
    "code": "from scipy.stats import trim_mean\n\n# Remove 10% from each tail (20% total)\ntrimmed_avg = trim_mean(df['value'], proportiontocut=0.1)\n\nprint(f'Trimmed Mean (10% each tail): {trimmed_avg:.2f}')\nprint(f'Regular Mean: {df[\"value\"].mean():.2f}')\nprint(f'Difference: {abs(trimmed_avg - df[\"value\"].mean()):.2f}')",
    "output_type": "None",
    "example": "# See snippet trimmed_mean"
  },
  "mad_statistic": {
    "id": "mad_statistic",
    "name": "Median Absolute Deviation (MAD)",
    "category": "statistics",
    "description": "Robust measure of variability that is resistant to outliers",
    "imports": "",
    "code": "from statsmodels import robust\n\n# Using statsmodels\nmad_value = robust.scale.mad(df['value'])\n\n# Manual calculation\nmedian_val = df['value'].median()\nmad_manual = abs(df['value'] - median_val).median() / 0.6744897501960817\n\nprint(f'MAD: {mad_value:.2f}')\nprint(f'Standard Deviation: {df[\"value\"].std():.2f}')\nprint(f'MAD is more robust to outliers')",
    "output_type": "None",
    "example": "# See snippet mad_statistic"
  },
  "permutation_test": {
    "id": "permutation_test",
    "name": "Permutation Test (Two Groups)",
    "category": "statistics",
    "description": "Non-parametric hypothesis test comparing two groups without assuming normality",
    "imports": "",
    "code": "import numpy as np\nimport random\n\ndef permutation_test(group_a, group_b, n_permutations=10000):\n    # Observed difference\n    observed_diff = np.mean(group_b) - np.mean(group_a)\n    \n    # Combine groups\n    combined = np.concatenate([group_a, group_b])\n    n_a, n_b = len(group_a), len(group_b)\n    \n    # Permutation distribution\n    perm_diffs = []\n    for _ in range(n_permutations):\n        shuffled = np.random.permutation(combined)\n        perm_a = shuffled[:n_a]\n        perm_b = shuffled[n_a:]\n        perm_diffs.append(np.mean(perm_b) - np.mean(perm_a))\n    \n    # P-value\n    p_value = np.mean(np.abs(perm_diffs) >= np.abs(observed_diff))\n    \n    return {'observed_diff': observed_diff, 'p_value': p_value}\n\nresult = permutation_test(df[df['group']=='A']['value'].values, \n                          df[df['group']=='B']['value'].values)\nprint(f\"Observed difference: {result['observed_diff']:.2f}\")\nprint(f\"P-value: {result['p_value']:.4f}\")",
    "output_type": "None",
    "example": "# See snippet permutation_test"
  },
  "correlation_ellipse": {
    "id": "correlation_ellipse",
    "name": "Correlation Matrix with Ellipses",
    "category": "visualization",
    "description": "Visualize correlation matrix using ellipses (grayscale-friendly)",
    "imports": "",
    "code": "from matplotlib.collections import EllipseCollection\nfrom matplotlib.colors import Normalize\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef plot_corr_ellipses(data, figsize=None, **kwargs):\n    M = np.array(data)\n    fig, ax = plt.subplots(1, 1, figsize=figsize, subplot_kw={'aspect': 'equal'})\n    ax.set_xlim(-0.5, M.shape[1] - 0.5)\n    ax.set_ylim(-0.5, M.shape[0] - 0.5)\n    ax.invert_yaxis()\n    xy = np.indices(M.shape)[::-1].reshape(2, -1).T\n    w = np.ones_like(M).ravel() + 0.01\n    h = 1 - np.abs(M).ravel() - 0.01\n    a = 45 * np.sign(M).ravel()\n    ec = EllipseCollection(widths=w, heights=h, angles=a, units='x', offsets=xy,\n                          norm=Normalize(vmin=-1, vmax=1),\n                          transOffset=ax.transData, array=M.ravel(), **kwargs)\n    ax.add_collection(ec)\n    if isinstance(data, pd.DataFrame):\n        ax.set_xticks(np.arange(M.shape[1]))\n        ax.set_xticklabels(data.columns, rotation=90)\n        ax.set_yticks(np.arange(M.shape[0]))\n        ax.set_yticklabels(data.index)\n    return ec, ax\n\ncorr_matrix = df.corr()\nm, ax = plot_corr_ellipses(corr_matrix, figsize=(8, 8), cmap='bwr_r')\nplt.colorbar(m, ax=ax, label='Correlation coefficient')\nplt.show()",
    "output_type": "None",
    "example": "# See snippet correlation_ellipse"
  },
  "partial_residual_plot": {
    "id": "partial_residual_plot",
    "name": "Partial Residual Plot",
    "category": "regression",
    "description": "Diagnostic plot for detecting nonlinearity in individual predictors",
    "imports": "",
    "code": "import statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# For linear terms, use statsmodels built-in\nfig, ax = plt.subplots(figsize=(8, 6))\nsm.graphics.plot_ccpr(model_result, 'predictor_name', ax=ax)\nplt.title('Partial Residual Plot')\nplt.show()\n\n# For polynomial terms, use custom implementation\n# See assets/harvest/scripts/partial_residual_plot.py",
    "output_type": "None",
    "example": "# See snippet partial_residual_plot"
  },
  "pca_scree_plot": {
    "id": "pca_scree_plot",
    "name": "PCA Scree Plot",
    "category": "pca",
    "description": "Visualize explained variance ratio for Principal Component Analysis",
    "imports": "",
    "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\nvar_exp = pca.explained_variance_ratio_\ncum_var_exp = np.cumsum(var_exp)\n\nplt.bar(range(1, len(var_exp) + 1), var_exp, alpha=0.5, align='center', label='Individual')\nplt.step(range(1, len(var_exp) + 1), cum_var_exp, where='mid', label='Cumulative')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()",
    "output_type": "None",
    "example": "# See snippet pca_scree_plot"
  },
  "pca_loadings_chart": {
    "id": "pca_loadings_chart",
    "name": "PCA Component Loadings",
    "category": "pca",
    "description": "Visualize feature contributions (loadings) to principal components",
    "imports": "",
    "code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Get loadings for first 2 components\nloadings = pd.DataFrame(pca.components_[0:2].T, \n                        index=df.columns, \n                        columns=['PC1', 'PC2'])\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\nloadings['PC1'].plot.bar(ax=ax1, color='blue')\nloadings['PC2'].plot.bar(ax=ax2, color='red')\nax1.set_ylabel('PC1 Loading')\nax2.set_ylabel('PC2 Loading')\nplt.tight_layout()\nplt.show()",
    "output_type": "None",
    "example": "# See snippet pca_loadings_chart"
  },
  "gower_distance_mixed": {
    "id": "gower_distance_mixed",
    "name": "Clustering Mixed Data (Gower)",
    "category": "clustering",
    "description": "Use Gower distance for clustering datasets with mixed categorical and numeric types",
    "imports": "",
    "code": "import gower\nfrom scipy.cluster.hierarchy import linkage, fcluster\nfrom scipy.spatial.distance import squareform\n\n# Calculate Gower distance matrix\ndist_matrix = gower.gower_matrix(df)\n\n# Hierarchical clustering\nZ = linkage(squareform(dist_matrix), method='complete')\nclusters = fcluster(Z, 4, criterion='maxclust')\n\ndf['cluster'] = clusters\nprint(df.groupby('cluster').mean())",
    "output_type": "None",
    "example": "# See snippet gower_distance_mixed"
  },
  "quantile_residual_grouping": {
    "id": "quantile_residual_grouping",
    "name": "High-Cardinality Grouping",
    "category": "feature-engineering",
    "description": "Group high-cardinality categorical levels (like zip codes) into quantiles based on model residuals",
    "imports": "",
    "code": "import pandas as pd\nimport numpy as np\n\n# 1. Calculate residuals for initial small model\nresiduals = y - model.predict(X)\n\n# 2. Group by categorical level and get median residual\nresid_df = pd.DataFrame({'category': df['zip_code'], 'resid': residuals})\nmedian_resids = resid_df.groupby('category')['resid'].median().sort_values()\n\n# 3. Create quantiles (e.g., quintiles)\nmedian_resids_df = median_resids.to_frame()\nmedian_resids_df['group'] = pd.qcut(median_resids_df['resid'], 5, labels=False)\n\n# 4. Map back to original dataframe\ndf['zip_group'] = df['zip_code'].map(median_resids_df['group'])",
    "output_type": "None",
    "example": "# See snippet quantile_residual_grouping"
  },
  "glm_partial_residual": {
    "id": "glm_partial_residual",
    "category": "visualization",
    "subcategory": "diagnostics",
    "name": "GLM Partial Residual Plot",
    "description": "Create partial residual plot for logistic regression with non-linear terms",
    "code": "import statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# Fit GLM with splines\nformula = 'outcome ~ bs(feature, df=8) + other_vars'\nmodel = smf.glm(formula=formula, data=df, family=sm.families.Binomial())\nresults = model.fit()\n\n# Create partial residual plot\nfig, ax = plt.subplots(figsize=(5, 5))\nglm_partial_residual_plot(results, df, 'outcome', 'feature', fig, ax)\nplt.show()",
    "tags": [
      "glm",
      "logistic",
      "diagnostics",
      "partial-residual",
      "splines"
    ]
  },
  "permutation_importance": {
    "id": "permutation_importance",
    "category": "machine_learning",
    "subcategory": "feature_selection",
    "name": "Permutation Feature Importance",
    "description": "Calculate model-agnostic feature importance via permutation",
    "code": "from sklearn.ensemble import RandomForestClassifier\n\n# Train model\nrf = RandomForestClassifier(n_estimators=500)\nrf.fit(X_train, y_train)\n\n# Calculate permutation importance\nimportance_df = calculate_permutation_importance(\n    rf, X, y, n_repeats=5, test_size=0.3\n)\n\n# Plot results\nfig, ax = plot_permutation_importance(importance_df, top_n=15)\nplt.show()",
    "tags": [
      "feature-importance",
      "permutation",
      "model-agnostic",
      "ml"
    ]
  },
  "smote_resampling": {
    "id": "smote_resampling",
    "category": "preprocessing",
    "subcategory": "imbalanced_data",
    "name": "SMOTE Oversampling",
    "description": "Handle imbalanced data using SMOTE synthetic sampling",
    "code": "from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\n# Split data first\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, stratify=y, random_state=42\n)\n\n# Apply SMOTE only to training data\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# Train model on resampled data\nmodel.fit(X_train_resampled, y_train_resampled)\n\n# Evaluate on original test distribution\ny_pred = model.predict(X_test)",
    "tags": [
      "imbalanced",
      "smote",
      "oversampling",
      "classification"
    ]
  },
  "class_weighting": {
    "id": "class_weighting",
    "category": "machine_learning",
    "subcategory": "imbalanced_data",
    "name": "Class Weighting for Imbalanced Data",
    "description": "Handle imbalanced data using sample weights",
    "code": "from sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Calculate weight for minority class\nminority_weight = 1 / np.mean(y == 'minority_class')\nsample_weights = [minority_weight if yi == 'minority_class' else 1 for yi in y]\n\n# Train with weights\nmodel = LogisticRegression()\nmodel.fit(X, y, sample_weight=sample_weights)\n\n# Or use automatic balancing\nmodel_auto = LogisticRegression(class_weight='balanced')\nmodel_auto.fit(X, y)",
    "tags": [
      "imbalanced",
      "class-weight",
      "classification",
      "logistic"
    ]
  },
  "roc_curve_plot": {
    "id": "roc_curve_plot",
    "category": "visualization",
    "subcategory": "classification",
    "name": "ROC Curve with AUC",
    "description": "Plot ROC curve for binary classification with AUC score",
    "code": "from sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# Get predicted probabilities\ny_pred_proba = model.predict_proba(X_test)[:, 1]\n\n# Calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba, pos_label='positive')\nauc_score = roc_auc_score(y_test, y_pred_proba)\n\n# Plot\nfig, ax = plt.subplots(figsize=(6, 6))\nax.plot(fpr, tpr, label=f'ROC (AUC = {auc_score:.3f})')\nax.plot([0, 1], [0, 1], 'k--', label='Random')\nax.set_xlabel('False Positive Rate (1 - Specificity)')\nax.set_ylabel('True Positive Rate (Recall)')\nax.set_title('ROC Curve')\nax.legend()\nplt.show()",
    "tags": [
      "roc",
      "auc",
      "classification",
      "evaluation"
    ]
  },
  "kmeans_elbow": {
    "id": "kmeans_elbow",
    "category": "machine_learning",
    "subcategory": "clustering",
    "name": "K-Means Elbow Method",
    "description": "Determine optimal number of clusters using elbow method",
    "code": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Calculate inertia for different k values\ninertias = []\nk_range = range(2, 15)\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n    kmeans.fit(X)\n    inertias.append(kmeans.inertia_ / k)\n\n# Plot elbow curve\nfig, ax = plt.subplots(figsize=(6, 4))\nax.plot(k_range, inertias, marker='o')\nax.set_xlabel('Number of Clusters (k)')\nax.set_ylabel('Average Within-Cluster Sum of Squares')\nax.set_title('Elbow Method for Optimal k')\nplt.show()",
    "tags": [
      "kmeans",
      "clustering",
      "elbow",
      "unsupervised"
    ]
  },
  "hierarchical_dendrogram": {
    "id": "hierarchical_dendrogram",
    "category": "visualization",
    "subcategory": "clustering",
    "name": "Hierarchical Clustering Dendrogram",
    "description": "Create dendrogram for hierarchical clustering",
    "code": "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\nimport matplotlib.pyplot as plt\n\n# Perform hierarchical clustering\nZ = linkage(X, method='ward')\n\n# Plot dendrogram\nfig, ax = plt.subplots(figsize=(10, 6))\ndendrogram(Z, labels=X.index, ax=ax)\nax.set_ylabel('Distance')\nax.set_title('Hierarchical Clustering Dendrogram')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n\n# Cut dendrogram to get clusters\nclusters = fcluster(Z, t=4, criterion='maxclust')",
    "tags": [
      "hierarchical",
      "clustering",
      "dendrogram",
      "unsupervised"
    ]
  }
}