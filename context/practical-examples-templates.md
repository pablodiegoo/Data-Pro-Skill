# Guia Pr√°tico: Templates e Exemplos de C√≥digo

## Parte 2: Implementa√ß√£o Detalhada com Exemplos Funcionais

---

## üî• Exemplo Completo: Dashboard de An√°lise de E-commerce

### 1. Arquivo de Configura√ß√£o

```python
# src/config/settings.py
from pathlib import Path
from dataclasses import dataclass
import os
from dotenv import load_dotenv

load_dotenv()

@dataclass
class Config:
    """Configura√ß√µes globais da aplica√ß√£o."""
    
    # Paths
    PROJECT_ROOT = Path(__file__).parent.parent.parent
    DATA_RAW = PROJECT_ROOT / "data" / "raw"
    DATA_PROCESSED = PROJECT_ROOT / "data" / "processed"
    OUTPUT_REPORTS = PROJECT_ROOT / "output" / "reports"
    OUTPUT_VISUALIZATIONS = PROJECT_ROOT / "output" / "visualizations"
    
    # Ensure directories exist
    for path in [DATA_RAW, DATA_PROCESSED, OUTPUT_REPORTS, OUTPUT_VISUALIZATIONS]:
        path.mkdir(parents=True, exist_ok=True)
    
    # Visualization
    PLOT_STYLE = 'seaborn-v0_8-darkgrid'
    PLOT_PALETTE = 'husl'
    FIG_DPI = 300
    FIG_SIZE_DEFAULT = (12, 6)
    FIG_SIZE_HEATMAP = (14, 10)
    
    # Analysis
    CORRELATION_THRESHOLD = 0.7
    OUTLIER_IQR_MULTIPLIER = 1.5
    
    # Reporting
    REPORT_AUTHOR = "Data Analysis Team"
    REPORT_COMPANY = "Your Company"

config = Config()
```

### 2. Carregamento de Dados com Valida√ß√£o

```python
# src/data/loaders.py (expandido)
import pandas as pd
import polars as pl
from pathlib import Path
from typing import Union, Optional
import logging

logger = logging.getLogger(__name__)

class DataLoader:
    """Carregador robusto de dados com valida√ß√£o."""
    
    @staticmethod
    def load_csv_safe(filepath: Union[str, Path], 
                     encoding: str = 'utf-8',
                     dtype_mapping: Optional[dict] = None,
                     **kwargs) -> pd.DataFrame:
        """
        Carrega CSV com tratamento robusto de erros.
        
        Args:
            filepath: Caminho do arquivo
            encoding: Encoding (default: utf-8)
            dtype_mapping: Dict de tipos de dados esperados
            **kwargs: Argumentos adicionais para pd.read_csv
        
        Returns:
            DataFrame carregado
        """
        try:
            df = pd.read_csv(filepath, encoding=encoding, **kwargs)
            
            # Aplicar tipos de dados se fornecidos
            if dtype_mapping:
                df = df.astype(dtype_mapping, errors='ignore')
            
            logger.info(f"‚úì Carregado {filepath}: {df.shape[0]} linhas, {df.shape[1]} colunas")
            return df
            
        except FileNotFoundError:
            logger.error(f"‚úó Arquivo n√£o encontrado: {filepath}")
            raise
        except Exception as e:
            logger.error(f"‚úó Erro ao carregar {filepath}: {str(e)}")
            raise
    
    @staticmethod
    def load_multiple_csv(directory: Union[str, Path], 
                         pattern: str = "*.csv") -> dict:
        """Carrega m√∫ltiplos CSVs de um diret√≥rio."""
        directory = Path(directory)
        files = list(directory.glob(pattern))
        
        data = {}
        for file in files:
            df = DataLoader.load_csv_safe(file)
            data[file.stem] = df
            
        logger.info(f"‚úì Carregados {len(data)} arquivos")
        return data
    
    @staticmethod
    def load_parquet(filepath: Union[str, Path]) -> pd.DataFrame:
        """Carrega Parquet (mais eficiente para grandes volumes)."""
        df = pd.read_parquet(filepath)
        logger.info(f"‚úì Carregado Parquet: {df.shape}")
        return df
    
    @staticmethod
    def validate_schema(df: pd.DataFrame, 
                       required_columns: list,
                       required_types: dict) -> bool:
        """
        Valida schema do DataFrame.
        
        Args:
            df: DataFrame
            required_columns: Colunas obrigat√≥rias
            required_types: {'column_name': 'int64', ...}
        
        Returns:
            True se v√°lido
        """
        # Check columns
        missing = set(required_columns) - set(df.columns)
        if missing:
            raise ValueError(f"Colunas faltando: {missing}")
        
        # Check types
        for col, dtype in required_types.items():
            if str(df[col].dtype) != dtype:
                logger.warning(f"Tipo de {col}: esperado {dtype}, obtido {df[col].dtype}")
        
        logger.info("‚úì Schema validado")
        return True
```

### 3. Limpeza e Transforma√ß√£o Avan√ßada

```python
# src/data/cleaners.py (expandido)
import pandas as pd
import numpy as np
from typing import List, Literal
import logging

logger = logging.getLogger(__name__)

class DataCleaner:
    """Limpeza avan√ßada e transforma√ß√£o de dados."""
    
    @staticmethod
    def comprehensive_cleaning(df: pd.DataFrame,
                              handle_missing: str = 'mean',
                              handle_outliers: bool = True,
                              normalize_text: bool = False) -> pd.DataFrame:
        """Pipeline completo de limpeza."""
        
        logger.info("Iniciando limpeza de dados...")
        df_clean = df.copy()
        
        # 1. Remove duplicatas
        duplicates = df_clean.duplicated().sum()
        if duplicates > 0:
            df_clean = df_clean.drop_duplicates()
            logger.info(f"  ‚úì Removidas {duplicates} duplicatas")
        
        # 2. Trata valores nulos
        nulls = df_clean.isnull().sum().sum()
        if nulls > 0:
            df_clean = DataCleaner.handle_missing_values(df_clean, strategy=handle_missing)
            logger.info(f"  ‚úì Tratados {nulls} valores nulos")
        
        # 3. Detecta e trata outliers
        if handle_outliers:
            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                outliers = DataCleaner.detect_outliers(df_clean[col], method='iqr')
                if outliers.sum() > 0:
                    logger.info(f"  ‚úì {outliers.sum()} outliers em {col}")
        
        # 4. Normaliza texto
        if normalize_text:
            text_cols = df_clean.select_dtypes(include=['object']).columns
            df_clean = DataCleaner.normalize_text(df_clean, list(text_cols))
            logger.info(f"  ‚úì Texto normalizado ({len(text_cols)} colunas)")
        
        logger.info("‚úì Limpeza completada")
        return df_clean
    
    @staticmethod
    def handle_missing_values(df: pd.DataFrame,
                             strategy: Literal['mean', 'median', 'mode', 'forward_fill', 'drop'] = 'mean',
                             threshold: float = 0.5) -> pd.DataFrame:
        """
        Trata valores nulos com m√∫ltiplas estrat√©gias.
        
        Args:
            df: DataFrame
            strategy: Estrat√©gia de preenchimento
            threshold: Se % nulos > threshold, remover coluna
        """
        df = df.copy()
        
        # Remove colunas muito vazias
        null_pct = df.isnull().sum() / len(df)
        cols_to_drop = null_pct[null_pct > threshold].index
        if len(cols_to_drop) > 0:
            df = df.drop(columns=cols_to_drop)
            logger.warning(f"Colunas removidas (>50% nulos): {list(cols_to_drop)}")
        
        # Aplicar estrat√©gia
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        object_cols = df.select_dtypes(include=['object']).columns
        
        if strategy == 'drop':
            return df.dropna()
        elif strategy == 'mean':
            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())
            df[object_cols] = df[object_cols].fillna(df[object_cols].mode().iloc[0])
        elif strategy == 'median':
            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
        elif strategy == 'forward_fill':
            df = df.fillna(method='ffill')
        
        return df
    
    @staticmethod
    def detect_outliers(series: pd.Series, 
                       method: Literal['iqr', 'zscore'] = 'iqr') -> pd.Series:
        """Detecta outliers usando IQR ou Z-Score."""
        if method == 'iqr':
            Q1 = series.quantile(0.25)
            Q3 = series.quantile(0.75)
            IQR = Q3 - Q1
            return (series < (Q1 - 1.5 * IQR)) | (series > (Q3 + 1.5 * IQR))
        else:  # zscore
            from scipy import stats
            return np.abs(stats.zscore(series)) > 3
    
    @staticmethod
    def normalize_text(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
        """Normaliza texto."""
        df = df.copy()
        for col in columns:
            df[col] = (df[col]
                      .str.lower()
                      .str.strip()
                      .str.replace(r'[^\w\s]', '', regex=True))
        return df
    
    @staticmethod
    def convert_datetime(df: pd.DataFrame, 
                        date_columns: dict) -> pd.DataFrame:
        """
        Converte para datetime.
        
        Args:
            date_columns: {'column_name': 'format_string', ...}
                         Ex: {'date': '%d/%m/%Y', 'timestamp': '%Y-%m-%d %H:%M:%S'}
        """
        df = df.copy()
        for col, fmt in date_columns.items():
            try:
                df[col] = pd.to_datetime(df[col], format=fmt)
            except Exception as e:
                logger.warning(f"Erro ao converter {col}: {e}")
        return df
```

### 4. An√°lises Estat√≠sticas Profissionais

```python
# src/analysis/statistical_tests.py
import pandas as pd
import numpy as np
from scipy import stats
from typing import Dict, Tuple
import logging

logger = logging.getLogger(__name__)

class StatisticalAnalysis:
    """An√°lises estat√≠sticas profissionais."""
    
    @staticmethod
    def comprehensive_analysis(df: pd.DataFrame,
                              target_col: str = None) -> Dict:
        """An√°lise completa de um dataset."""
        
        results = {
            'shape': df.shape,
            'basic_stats': DescriptiveAnalysis.summary_statistics(df).to_dict(),
            'correlations': DescriptiveAnalysis.correlation_matrix(df).to_dict(),
            'missing_values': df.isnull().sum().to_dict(),
            'data_types': df.dtypes.astype(str).to_dict(),
        }
        
        # An√°lise por coluna
        for col in df.select_dtypes(include=[np.number]).columns:
            results[f'{col}_distribution'] = {
                'skewness': stats.skew(df[col].dropna()),
                'kurtosis': stats.kurtosis(df[col].dropna()),
                'normality_test': StatisticalAnalysis.normality_test(df[col])
            }
        
        return results
    
    @staticmethod
    def normality_test(series: pd.Series,
                      alpha: float = 0.05) -> Dict[str, bool]:
        """
        Testa normalidade com m√∫ltiplos testes.
        
        Returns:
            Dict com resultados dos testes
        """
        data = series.dropna()
        
        # Shapiro-Wilk (melhor para n < 5000)
        if len(data) < 5000:
            shapiro_stat, shapiro_p = stats.shapiro(data)
            shapiro_normal = shapiro_p > alpha
        else:
            shapiro_stat = shapiro_p = shapiro_normal = None
        
        # Kolmogorov-Smirnov
        ks_stat, ks_p = stats.kstest(data, 'norm', 
                                     args=(data.mean(), data.std()))
        ks_normal = ks_p > alpha
        
        # Anderson-Darling
        anderson_result = stats.anderson(data, dist='norm')
        anderson_normal = anderson_result.statistic < anderson_result.critical_values[2]
        
        return {
            'shapiro_wilk': shapiro_normal,
            'kolmogorov_smirnov': ks_normal,
            'anderson_darling': anderson_normal,
            'is_normal': all([ks_normal, anderson_normal])
        }
    
    @staticmethod
    def correlation_analysis(df: pd.DataFrame,
                            min_correlation: float = 0.5) -> Dict:
        """An√°lise de correla√ß√µes com filtro."""
        corr_matrix = df.corr(numeric_only=True)
        
        # Encontra correla√ß√µes fortes
        strong_corr = {}
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) >= min_correlation:
                    col1 = corr_matrix.columns[i]
                    col2 = corr_matrix.columns[j]
                    strong_corr[f"{col1} <-> {col2}"] = corr_val
        
        return {
            'strong_correlations': strong_corr,
            'correlation_matrix': corr_matrix.to_dict()
        }
    
    @staticmethod
    def hypothesis_test(group1: pd.Series,
                       group2: pd.Series,
                       test_type: str = 'ttest') -> Dict:
        """
        Testes de hip√≥tese entre dois grupos.
        
        test_type: 'ttest' (param√©trico) ou 'mannwhitney' (n√£o-param√©trico)
        """
        
        if test_type == 'ttest':
            stat, p_value = stats.ttest_ind(group1.dropna(), group2.dropna())
            test_name = "T-Test de Student"
        else:  # mann-whitney
            stat, p_value = stats.mannwhitneyu(group1.dropna(), group2.dropna())
            test_name = "Mann-Whitney U Test"
        
        alpha = 0.05
        significant = p_value < alpha
        
        return {
            'test': test_name,
            'statistic': stat,
            'p_value': p_value,
            'significant': significant,
            'interpretation': "Diferen√ßa significativa" if significant else "Sem diferen√ßa significativa"
        }
    
    @staticmethod
    def anova_test(groups: list) -> Dict:
        """ANOVA: compara m√∫ltiplos grupos."""
        f_stat, p_value = stats.f_oneway(*groups)
        
        return {
            'test': 'ANOVA F-Test',
            'f_statistic': f_stat,
            'p_value': p_value,
            'significant': p_value < 0.05
        }
```

### 5. Visualiza√ß√µes Avan√ßadas

```python
# src/visualization/advanced_plots.py
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np
from typing import Optional, List

class AdvancedVisualizations:
    """Visualiza√ß√µes profissionais e interativas."""
    
    def __init__(self, style: str = 'seaborn-v0_8-darkgrid'):
        sns.set_style(style)
        self.colors = sns.color_palette("husl", 8)
    
    def multi_panel_dashboard(self,
                             df: pd.DataFrame,
                             numeric_cols: List[str]) -> plt.Figure:
        """Dashboard com m√∫ltiplos pain√©is."""
        
        n_cols = min(len(numeric_cols), 3)
        n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
        axes = axes.flatten()
        
        for idx, col in enumerate(numeric_cols):
            # Histograma + KDE
            axes[idx].hist(df[col].dropna(), bins=30, alpha=0.7, 
                          color=self.colors[idx], edgecolor='black')
            ax2 = axes[idx].twinx()
            df[col].plot.kde(ax=ax2, color='red', linewidth=2)
            
            axes[idx].set_title(f'Distribui√ß√£o: {col}', fontweight='bold')
            axes[idx].set_xlabel(col)
            axes[idx].set_ylabel('Frequ√™ncia')
        
        # Remove eixos n√£o utilizados
        for idx in range(len(numeric_cols), len(axes)):
            fig.delaxes(axes[idx])
        
        plt.tight_layout()
        return fig
    
    def heatmap_with_annotations(self,
                                df: pd.DataFrame,
                                title: str = "Correlation Matrix") -> plt.Figure:
        """Heatmap profissional com anota√ß√µes."""
        
        fig, ax = plt.subplots(figsize=(12, 10))
        
        corr = df.corr(numeric_only=True)
        
        # M√°scara para tri√¢ngulo superior
        mask = np.triu(np.ones_like(corr, dtype=bool), k=1)
        
        sns.heatmap(corr, mask=mask, annot=True, fmt='.2f',
                   cmap='RdBu_r', center=0, square=True, ax=ax,
                   cbar_kws={'label': 'Correla√ß√£o', 'shrink': 0.8},
                   linewidths=0.5, vmin=-1, vmax=1)
        
        ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
        
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        plt.tight_layout()
        return fig
    
    def time_series_decomposition(self,
                                 series: pd.Series,
                                 title: str = "Series Decomposition") -> plt.Figure:
        """Decomp√µe s√©rie temporal em tend√™ncia + sazonalidade."""
        
        from statsmodels.tsa.seasonal import seasonal_decompose
        
        decomposition = seasonal_decompose(series, model='additive', period=12)
        
        fig, axes = plt.subplots(4, 1, figsize=(14, 10))
        
        series.plot(ax=axes[0], color='blue', linewidth=2)
        axes[0].set_ylabel('Original')
        
        decomposition.trend.plot(ax=axes[1], color='orange', linewidth=2)
        axes[1].set_ylabel('Trend')
        
        decomposition.seasonal.plot(ax=axes[2], color='green', linewidth=2)
        axes[2].set_ylabel('Seasonal')
        
        decomposition.resid.plot(ax=axes[3], color='red', linewidth=2)
        axes[3].set_ylabel('Residual')
        
        fig.suptitle(title, fontsize=16, fontweight='bold')
        plt.tight_layout()
        return fig
    
    @staticmethod
    def interactive_scatter_3d(df: pd.DataFrame,
                              x: str, y: str, z: str,
                              color: Optional[str] = None,
                              title: str = "3D Scatter Plot") -> go.Figure:
        """Gr√°fico 3D interativo com Plotly."""
        
        fig = px.scatter_3d(df, x=x, y=y, z=z, color=color,
                           hover_name=df.index,
                           title=title,
                           labels={x: x, y: y, z: z})
        
        fig.update_traces(marker=dict(size=5, opacity=0.7))
        
        return fig
    
    @staticmethod
    def interactive_dashboard(df: pd.DataFrame,
                             dimensions: dict) -> go.Figure:
        """Dashboard interativo com Plotly Sunburst."""
        
        # dimensions = {'category': 'col1', 'subcategory': 'col2', 'values': 'col3'}
        
        fig = px.sunburst(df,
                         names=dimensions.get('names'),
                         parents=dimensions.get('parents'),
                         values=dimensions.get('values'),
                         title="Interactive Dashboard")
        
        return fig
```

### 6. Template de Relat√≥rio Din√¢mico

```python
# src/reporting/report_templates.py
from jinja2 import Template
from datetime import datetime

class ReportTemplates:
    """Templates para diferentes tipos de relat√≥rios."""
    
    EXECUTIVE_SUMMARY = """
# Relat√≥rio Executivo: {{ title }}

**Data de Gera√ß√£o:** {{ date }}  
**Autor:** {{ author }}  
**Per√≠odo:** {{ period }}

---

## üìä Resumo Executivo

{{ executive_summary }}

### M√©tricas Principais

| M√©trica | Valor | Varia√ß√£o |
|---------|-------|----------|
{% for metric, value, change in metrics %}
| {{ metric }} | {{ value }} | {{ change }} |
{% endfor %}

### Destaques

{% for highlight in highlights %}
- ‚úì {{ highlight }}
{% endfor %}

---

## üîç An√°lise Detalhada

### Distribui√ß√£o de Dados

![Distribution]({{ dist_image }})

*Figura 1: Distribui√ß√£o dos principais dados*

### Correla√ß√µes

![Correlations]({{ corr_image }})

*Figura 2: Matriz de correla√ß√£o*

### S√©rie Temporal

![TimeSeries]({{ ts_image }})

*Figura 3: Evolu√ß√£o ao longo do tempo*

---

## üìà Insights e Recomenda√ß√µes

{% for insight in insights %}
### {{ insight.title }}

{{ insight.description }}

**Recomenda√ß√£o:** {{ insight.recommendation }}

{% endfor %}

---

## üìã Dados Adicionais

{{ additional_data }}

---

## Conclus√£o

{{ conclusion }}

**Pr√≥ximos Passos:**
{% for step in next_steps %}
{{ loop.index }}. {{ step }}
{% endfor %}

---

_Relat√≥rio gerado automaticamente em {{ generated_date }}_
"""
    
    @staticmethod
    def render_report(template_str: str, **context) -> str:
        """Renderiza template com contexto."""
        template = Template(template_str)
        return template.render(**context, date=datetime.now().strftime("%d/%m/%Y"))
```

### 7. Script de Gera√ß√£o Completa

```python
# scripts/generate_ecommerce_report.py
#!/usr/bin/env python
"""Script completo: gera an√°lise e relat√≥rio de e-commerce."""

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
from pathlib import Path
from src.config.settings import config
from src.data.loaders import DataLoader
from src.data.cleaners import DataCleaner
from src.analysis.statistical_tests import StatisticalAnalysis
from src.visualization.advanced_plots import AdvancedVisualizations
from src.visualization.mermaid_diagrams import MermaidDiagrams
from src.reporting.report_generator import ReportGenerator
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """Pipeline completo de an√°lise."""
    
    logger.info("="*60)
    logger.info("Iniciando gera√ß√£o de relat√≥rio de E-commerce")
    logger.info("="*60)
    
    # 1. Carregar dados
    logger.info("\n[1/7] Carregando dados...")
    try:
        df = DataLoader.load_csv_safe(
            config.DATA_RAW / "ecommerce_sales.csv",
            dtype_mapping={
                'order_id': 'int64',
                'order_date': 'str',
                'customer_id': 'int64',
                'revenue': 'float64'
            }
        )
    except Exception as e:
        logger.error(f"Erro ao carregar dados: {e}")
        return
    
    # 2. Converter datas
    logger.info("[2/7] Processando datas...")
    df = DataCleaner.convert_datetime(df, {
        'order_date': '%Y-%m-%d'
    })
    
    # 3. Limpeza completa
    logger.info("[3/7] Limpando dados...")
    df_clean = DataCleaner.comprehensive_cleaning(
        df,
        handle_missing='mean',
        handle_outliers=True
    )
    
    # 4. An√°lise estat√≠stica
    logger.info("[4/7] Realizando an√°lises estat√≠sticas...")
    stats = StatisticalAnalysis.comprehensive_analysis(df_clean)
    corr_results = StatisticalAnalysis.correlation_analysis(df_clean, min_correlation=0.5)
    
    # 5. Gerar visualiza√ß√µes
    logger.info("[5/7] Gerando visualiza√ß√µes...")
    viz = AdvancedVisualizations()
    
    # Dashboard m√∫ltiplo
    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()
    fig_dashboard = viz.multi_panel_dashboard(df_clean, numeric_cols[:6])
    viz_path_dashboard = config.OUTPUT_VISUALIZATIONS / "dashboard.png"
    fig_dashboard.savefig(viz_path_dashboard, dpi=config.FIG_DPI, bbox_inches='tight')
    
    # Heatmap
    fig_heatmap = viz.heatmap_with_annotations(df_clean, "Correla√ß√£o entre Vari√°veis")
    viz_path_heatmap = config.OUTPUT_VISUALIZATIONS / "correlations.png"
    fig_heatmap.savefig(viz_path_heatmap, dpi=config.FIG_DPI, bbox_inches='tight')
    
    # 6. Criar diagrama Mermaid
    logger.info("[6/7] Gerando diagramas...")
    pipeline_diagram = MermaidDiagrams.flowchart(
        nodes=[
            {"id": "A", "label": "Raw Data\n(CSV)"},
            {"id": "B", "label": "Data Cleaning"},
            {"id": "C", "label": "Statistical Analysis"},
            {"id": "D", "label": "Visualizations"},
            {"id": "E", "label": "Report Generation"},
        ],
        edges=[("A", "B"), ("B", "C"), ("C", "D"), ("D", "E")],
        title="E-commerce Analysis Pipeline"
    )
    
    # 7. Gerar relat√≥rio final
    logger.info("[7/7] Gerando relat√≥rio...")
    report = ReportGenerator(str(config.OUTPUT_REPORTS))
    
    report.add_title(
        "An√°lise Completa de Vendas E-commerce",
        f"Per√≠odo: {df_clean['order_date'].min().date()} a {df_clean['order_date'].max().date()}"
    )
    
    report.add_section("1. Resumo Executivo")
    report.add_text(f"""
    Este relat√≥rio apresenta uma an√°lise completa das vendas de e-commerce, 
    cobrindo {len(df_clean)} transa√ß√µes com receita total de R$ {df_clean['revenue'].sum():,.2f}.
    
    **Destaques:**
    - Ticket M√©dio: R$ {df_clean['revenue'].mean():.2f}
    - Receita M√≠nima: R$ {df_clean['revenue'].min():.2f}
    - Receita M√°xima: R$ {df_clean['revenue'].max():.2f}
    - Desvio Padr√£o: R$ {df_clean['revenue'].std():.2f}
    """)
    
    report.add_section("2. Pipeline de An√°lise")
    report.add_mermaid_diagram(pipeline_diagram)
    
    report.add_section("3. Estat√≠sticas Descritivas")
    summary_stats = df_clean[numeric_cols].describe().round(2)
    report.add_table(summary_stats, "Resumo Estat√≠stico")
    
    report.add_section("4. Dashboard de Distribui√ß√µes")
    report.add_figure(
        str(viz_path_dashboard),
        "Dashboard com distribui√ß√µes de principais vari√°veis"
    )
    
    report.add_section("5. An√°lise de Correla√ß√µes")
    report.add_figure(
        str(viz_path_heatmap),
        "Matriz de correla√ß√£o entre vari√°veis"
    )
    
    if corr_results['strong_correlations']:
        report.add_text("**Correla√ß√µes Fortes Encontradas:**\n")
        for pair, corr_value in corr_results['strong_correlations'].items():
            report.add_text(f"- {pair}: {corr_value:.3f}")
    
    report.add_section("6. Conclus√µes e Recomenda√ß√µes")
    report.add_text("""
    ### Principais Descobertas
    
    1. **Distribui√ß√£o de Receita**: Os dados mostram uma distribui√ß√£o aproximadamente normal
       com poucos outliers, indicando consist√™ncia nas opera√ß√µes.
    
    2. **Padr√µes de Vendas**: An√°lise temporal revela sazonalidade significativa,
       com picos em per√≠odos espec√≠ficos do ano.
    
    3. **Segmenta√ß√£o de Clientes**: Identificadas 3 grupos principais de clientes
       com padr√µes de compra distintos.
    
    ### Recomenda√ß√µes
    
    ‚úì Aumentar esfor√ßos de marketing nos per√≠odos de baixa demanda  
    ‚úì Desenvolver estrat√©gia de reten√ß√£o para clientes de alto valor  
    ‚úì Otimizar invent√°rio baseado em padr√µes sazonais  
    ‚úì Investigar outliers negativos para melhorar taxa de convers√£o
    """)
    
    # Salvar
    logger.info("\nSalvando relat√≥rios...")
    md_path = report.save_markdown("ecommerce_analysis.md")
    pdf_path = report.save_pdf("ecommerce_analysis.pdf")
    html_path = report.save_html("ecommerce_analysis.html")
    
    logger.info("\n" + "="*60)
    logger.info("‚úÖ Relat√≥rio gerado com sucesso!")
    logger.info(f"  üìÑ Markdown: {md_path}")
    logger.info(f"  üìë PDF: {pdf_path}")
    logger.info(f"  üåê HTML: {html_path}")
    logger.info("="*60)

if __name__ == "__main__":
    main()
```

---

## üéì Checklist de Implementa√ß√£o

```markdown
## Semana 1: Setup e Funda√ß√µes
- [ ] Criar estrutura de diret√≥rios
- [ ] Instalar depend√™ncias (requirements.txt)
- [ ] Configurar logging
- [ ] Criar scripts de teste
- [ ] Preparar dados amostrais

## Semana 2: M√≥dulos de Dados
- [ ] Implementar DataLoader
- [ ] Implementar DataCleaner
- [ ] Criar testes unit√°rios para loaders
- [ ] Documentar APIs
- [ ] Criar exemplos de uso

## Semana 3: An√°lises
- [ ] Implementar an√°lises descritivas
- [ ] Implementar testes estat√≠sticos
- [ ] Implementar detec√ß√£o de outliers
- [ ] Criar notebooks de explora√ß√£o
- [ ] Validar resultados

## Semana 4: Visualiza√ß√µes
- [ ] Implementar gr√°ficos est√°ticos
- [ ] Implementar gr√°ficos interativos
- [ ] Integrar Mermaid.js
- [ ] Criar galeria de exemplos
- [ ] Otimizar performance

## Semana 5: Relat√≥rios
- [ ] Implementar ReportGenerator
- [ ] Criar templates Jinja2
- [ ] Testar exporta√ß√£o PDF
- [ ] Testar exporta√ß√£o HTML
- [ ] Criar exemplos de relat√≥rios

## Semana 6: Integra√ß√£o e Deploy
- [ ] Pipeline end-to-end
- [ ] CI/CD (GitHub Actions)
- [ ] Docker containerization
- [ ] Documenta√ß√£o final
- [ ] Deploy piloto

## Semana 7-8: Refinement
- [ ] Testes de performance
- [ ] Otimiza√ß√µes
- [ ] User feedback
- [ ] Documenta√ß√£o avan√ßada
- [ ] Release v1.0
```

---

## üöÄ Comandos R√°pidos

```bash
# Setup inicial
git clone <repo>
cd data-analysis-platform
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Rodar an√°lise completa
python scripts/generate_ecommerce_report.py

# Rodar testes
pytest tests/ -v

# Gerar documenta√ß√£o
sphinx-build -b html docs/ docs/_build/

# Build Docker
docker build -t data-analysis:latest .
docker run -v $(pwd):/app data-analysis:latest
```

---

**Status:** ‚úÖ Pronto para Desenvolvimento | **Vers√£o:** 1.0 | **√öltima atualiza√ß√£o:** Janeiro 2026
